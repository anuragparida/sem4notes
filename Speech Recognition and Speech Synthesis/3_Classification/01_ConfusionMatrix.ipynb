{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54303a12",
   "metadata": {},
   "source": [
    "# Confusion matrix\n",
    "\n",
    "A confusion matrix counts the detected words depending on the spoken/tested words. It is a compact logging tool for a classification task/experiment.\n",
    "The spoken words corresponds to the rows of the matrix and the detected words corresponds to the columns of the matrix.\n",
    "\n",
    "In the following experiment, a logging of a confusion matrix is simulated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7020d55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55. 13. 12. 13.]\n",
      " [ 8. 70.  9. 10.]\n",
      " [ 8. 20. 54. 12.]\n",
      " [10. 13. 15. 59.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "NumberOfWordClasses = 4\n",
    "NumberOfWordsPerClass = 100\n",
    "AssumedAccuracy = 0.6\n",
    "AssumedFalseRejectionRate = 0.05\n",
    "\n",
    "NumberOfTotalWords = NumberOfWordClasses * NumberOfWordsPerClass\n",
    "FalseRejectionCounter = 0\n",
    "ConfusionMatrix = np.zeros((NumberOfWordClasses, NumberOfWordClasses))\n",
    "for SpokenWordIndex in range(ConfusionMatrix.shape[0]):\n",
    "    for n in range(NumberOfWordsPerClass):\n",
    "        RandomNumber = np.random.rand(1)\n",
    "        if RandomNumber < AssumedFalseRejectionRate:\n",
    "            # false rejection occurs\n",
    "            FalseRejectionCounter += 1\n",
    "        elif RandomNumber < AssumedFalseRejectionRate + AssumedAccuracy:\n",
    "            # correct classification occurs\n",
    "            ConfusionMatrix[SpokenWordIndex, SpokenWordIndex] += 1\n",
    "        else:\n",
    "            # wrong classification occurs\n",
    "            # find arbitrary wrong index\n",
    "            TargetWrongIndex = SpokenWordIndex\n",
    "            while TargetWrongIndex == SpokenWordIndex:\n",
    "                TargetWrongIndex = int(np.random.rand(1) * NumberOfWordClasses)\n",
    "            ConfusionMatrix[SpokenWordIndex, TargetWrongIndex] += 1\n",
    "            \n",
    "print(ConfusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c685e586",
   "metadata": {},
   "source": [
    "There exists several parameters for the confusion matrix in order to evaluate the goodness of the underlying experiment of classification:\n",
    "\n",
    "## Accuracy\n",
    "Accuracy is the number of correct detections (the main diagonal) over the total number of spoken words. The range for the accuracy is 0..1. An accuracy of 0 is the worst case scenario with zero correct detected words. A scenario with an accuracy of 1 is a perfect scenario, where all spoken words are detected correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b80e5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.56\n"
     ]
    }
   ],
   "source": [
    "NumberOfCorrectDetectedWords = 0\n",
    "for SpokenWordIndex in range(ConfusionMatrix.shape[0]):\n",
    "    NumberOfCorrectDetectedWords += ConfusionMatrix[SpokenWordIndex, SpokenWordIndex]\n",
    "Accuracy = NumberOfCorrectDetectedWords / NumberOfTotalWords\n",
    "print('accuracy = ', str(Accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecdfdd0",
   "metadata": {},
   "source": [
    "## False rejections\n",
    "\n",
    "Another parameter for evaluating a voice control algorithm is the false rejection rate. If a word is spoken and the voice detection algorithm ignores the spoken word, a false rejection occurs. The false rejection rate is the number of false rejections over the number of spoken/tested words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "477be273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of false rejections =  19\n",
      "false rejection rate =  0.0475\n"
     ]
    }
   ],
   "source": [
    "NumberOfFalseRejections = NumberOfTotalWords - np.sum(ConfusionMatrix)\n",
    "FalseRejectionRate = NumberOfFalseRejections / NumberOfTotalWords\n",
    "\n",
    "print('Number of false rejections = ', str(int(NumberOfFalseRejections)))\n",
    "print('false rejection rate = ', str(FalseRejectionRate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d0146c",
   "metadata": {},
   "source": [
    "## False alarms\n",
    "\n",
    "False alarms occurs, if no word is spoken, but the voice control system detects a word due to background noise or internal errors. In a real voice control benchmark, the false alarms must be counted in order to evaluate the voice control algorithm correctly. In this artificial scenario with a random confusion matrix, the false alarms must be set to a random value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51de0332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of false alarms =  0\n",
      "false alarm rate =  0.0\n"
     ]
    }
   ],
   "source": [
    "NumberOfFalseAlarms = int(2 * np.random.rand(1) * NumberOfTotalWords * AssumedFalseRejectionRate)\n",
    "FalseAlarmRate = NumberOfFalseAlarms / NumberOfTotalWords\n",
    "\n",
    "print('Number of false alarms = ', str(NumberOfFalseAlarms))\n",
    "print('false alarm rate = ', str(FalseAlarmRate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b99804a",
   "metadata": {},
   "source": [
    "## Word error rate\n",
    "\n",
    "In order to evaluate the overall performance of a classification task, the accuracy, the false alarms and the false rejections must be considered.\n",
    "\n",
    "To simply compare two or more different classification algorithms, a single value measurement is beneficial. One example for such a single value measurement is the word error rate. It is defined as the sum of all errors (wrong classifications, false rejections and false alarms) over the number of spoken/testet words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02ba8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word error rate =  0.4875\n"
     ]
    }
   ],
   "source": [
    "NumberOfWrongClassifications = NumberOfTotalWords - NumberOfCorrectDetectedWords\n",
    "NumberOfWordErrors = NumberOfFalseAlarms + NumberOfFalseRejections + NumberOfWrongClassifications\n",
    "WordErrorRate = NumberOfWordErrors / NumberOfTotalWords\n",
    "\n",
    "print('word error rate = ', str(WordErrorRate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ae30e",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "Precision is the relation, how often a single word is correct spoken (the entry on the main diagonal) to how often this word is detected (the sum of the column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b87ca672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for the  0 -th word:  0.5656565656565656\n",
      "Precision for the  1 -th word:  0.5517241379310345\n",
      "Precision for the  2 -th word:  0.59\n",
      "Precision for the  3 -th word:  0.6421052631578947\n"
     ]
    }
   ],
   "source": [
    "for n in range(ConfusionMatrix.shape[1]):\n",
    "    Precision = ConfusionMatrix[n, n] / np.sum(ConfusionMatrix[:, n])\n",
    "    print('Precision for the ', n, '-th word: ', Precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd2276",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "Recall is the relation, how often a single word is correct detected (the entry on the main diagonal) to how often this word is spoken/tested (the sum of the row):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0efaba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for the  0 -th word:  0.5957446808510638\n",
      "Recall for the  1 -th word:  0.5161290322580645\n",
      "Recall for the  2 -th word:  0.6210526315789474\n",
      "Recall for the  3 -th word:  0.6161616161616161\n"
     ]
    }
   ],
   "source": [
    "for n in range(ConfusionMatrix.shape[0]):\n",
    "    Recall = ConfusionMatrix[n, n] / np.sum(ConfusionMatrix[n, :])\n",
    "    print('Recall for the ', n, '-th word: ', Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9b875-549c-4cde-a7be-101ce1d12a5c",
   "metadata": {},
   "source": [
    "## Accuracy paradox\n",
    "Accuracy is only a practical measure for goodness of fit for the training data if the training data is nearly balanced.\n",
    "\n",
    "## Balanced trainingsdata\n",
    "Balanced trainingsdata means, that nearly all classes to be detectet has nearly the same amount of training samples. For imbalanced trainingsdata, the classes has not the same probability in the trainingsdata.\n",
    "\n",
    "A perfect dice will produce all six numbers nearly equally often. Therefore, each class ha a probability of roughly $\\frac{1}{6}$ in the trainingsdata.\n",
    "\n",
    "Trainingsdata for detecting earthquakes may be totally imbalanced, because in $99.99$ percent of all cases, there is no earthquake. Training with such a set of trainingsdata may result in the simplest possible classificator, which simply detects the class with the highest probability. In this case, the classificator always states: there is no earthquake. With this statement, an accuracy of $99.99$ percent is reached. But nevertheless, this classificator is useless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f7600c-1703-482a-9c67-a57c8dbd6f89",
   "metadata": {},
   "source": [
    "## F Score\n",
    "In the case of unbalanced data, the F is a better indicator for goodness of fit of the given classification than the accuracy.\n",
    "\n",
    "The F score is the harmonic mean of the mean precision and the mean recall:\n",
    "\n",
    "$F=\\frac{2}{\\frac{1}{\\text{precision}}+\\frac{1}{\\text{recall}}}$\n",
    "\n",
    "The bigger the F score, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc408b",
   "metadata": {},
   "source": [
    "## Programming Exercise:\n",
    "\n",
    "A camera system should detect if a traffic light is red.\n",
    "The following confusion matrix is measured.\n",
    "The first row corresponds to a traffic light showing 'red'.\n",
    "The second row corresponds to a traffic light showing 'green'.\n",
    "The first column corresponds to a detected traffic light 'red'.\n",
    "The second column corresponds to a detected traffic light 'green'.\n",
    "\n",
    "The aim is to detect red lights.\n",
    "By this, false alarms are defined by: A red light is detected but a green light would be correct.\n",
    "By this, false rejections are defined by: A green light is detected but a red light would be correct.\n",
    "\n",
    "Define the procedures to evaluate the accuracy, the mean precision, the mean recall, the false alarm rate, the false rejection rate and the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f8b3215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_Accuracy1 (__main__.TestProgrammingExercise.test_Accuracy1) ... ok\n",
      "test_Accuracy2 (__main__.TestProgrammingExercise.test_Accuracy2) ... ok\n",
      "test_F1Score1 (__main__.TestProgrammingExercise.test_F1Score1) ... ok\n",
      "test_F1Score2 (__main__.TestProgrammingExercise.test_F1Score2) ... ok\n",
      "test_FalseAlarms1 (__main__.TestProgrammingExercise.test_FalseAlarms1) ... ok\n",
      "test_FalseAlarms2 (__main__.TestProgrammingExercise.test_FalseAlarms2) ... ok\n",
      "test_FalseRejections1 (__main__.TestProgrammingExercise.test_FalseRejections1) ... ok\n",
      "test_FalseRejections2 (__main__.TestProgrammingExercise.test_FalseRejections2) ... ok\n",
      "test_Precision1 (__main__.TestProgrammingExercise.test_Precision1) ... ok\n",
      "test_Precision2 (__main__.TestProgrammingExercise.test_Precision2) ... ok\n",
      "test_Recall1 (__main__.TestProgrammingExercise.test_Recall1) ... ok\n",
      "test_Recall2 (__main__.TestProgrammingExercise.test_Recall2) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 0.019s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1f067909550>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def EvalAccuracy(ConfusionMatrix):\n",
    "    Accuracy = 0.0\n",
    "    # solution begins\n",
    "\n",
    "    # solution ends\n",
    "    return Accuracy\n",
    "\n",
    "def EvalPrecision(ConfusionMatrix):\n",
    "    MeanPrecision = 0.0\n",
    "    # solution begins\n",
    "\n",
    "    # solution ends\n",
    "    return MeanPrecision\n",
    "\n",
    "def EvalRecall(ConfusionMatrix):\n",
    "    MeanRecall = 0.0\n",
    "    # solution begins\n",
    "\n",
    "    # solution ends\n",
    "    return MeanRecall\n",
    "\n",
    "def EvalFalseAlarms(ConfusionMatrix):\n",
    "    FalseAlarms = 0.0\n",
    "    # solution begins\n",
    "\n",
    "    # solution ends\n",
    "    return FalseAlarms\n",
    "\n",
    "def EvalFalseRejections(ConfusionMatrix):\n",
    "    FalseRejections = 0.0\n",
    "    # solution begins\n",
    "\n",
    "    # solution ends\n",
    "    return FalseRejections\n",
    "\n",
    "def EvalF1Score(ConfusionMatrix):\n",
    "    F1Score = 0.0\n",
    "    # solution begins\n",
    "\n",
    "    # solution ends\n",
    "    return F1Score\n",
    "\n",
    "import unittest\n",
    "\n",
    "class TestProgrammingExercise(unittest.TestCase):\n",
    "\n",
    "    def test_Accuracy1(self):\n",
    "        ConfusionMatrix = np.array([[7, 2], [3, 6]])\n",
    "        accuracy = EvalAccuracy(ConfusionMatrix)\n",
    "        self.assertAlmostEqual(accuracy, 0.722, delta = 1e-3)\n",
    "\n",
    "    def test_Accuracy2(self):\n",
    "        ConfusionMatrix = np.array([[9, 5], [1, 4]])\n",
    "        accuracy = EvalAccuracy(ConfusionMatrix)\n",
    "        self.assertAlmostEqual(accuracy, 0.684, delta = 1e-3)\n",
    "\n",
    "    def test_Precision1(self):\n",
    "        ConfusionMatrix = np.array([[7, 2], [3, 6]])\n",
    "        precision = EvalPrecision(ConfusionMatrix)\n",
    "        self.assertAlmostEqual(precision, 0.725, delta = 1e-3)\n",
    "\n",
    "    def test_Precision2(self):\n",
    "        ConfusionMatrix = np.array([[9, 5], [1, 4]])\n",
    "        precision = EvalPrecision(ConfusionMatrix)\n",
    "        self.assertAlmostEqual(precision, 0.672, delta = 1e-3)\n",
    "\n",
    "    def test_Recall1(self):\n",
    "        ConfusionMatrix = np.array([[7, 2], [3, 6]])\n",
    "        recall = EvalRecall(ConfusionMatrix)\n",
    "        self.assertAlmostEqual(recall, 0.722, delta = 1e-3)\n",
    "\n",
    "    def test_Recall2(self):\n",
    "        ConfusionMatrix = np.array([[9, 5], [1, 4]])\n",
    "        recall = EvalRecall(ConfusionMatrix)\n",
    "        self.assertAlmostEqual(recall, 0.721, delta = 1e-3)\n",
    "\n",
    "    def test_FalseAlarms1(self):\n",
    "        ConfusionMatrix = np.array([[7, 2], [3, 6]])\n",
    "        FalseAlarms = EvalFalseAlarms(ConfusionMatrix)\n",
    "        self.assertAlmostEqual(FalseAlarms, 0.111, delta = 1e-3)\n",
    "\n",
    "    def test_FalseAlarms2(self):\n",
    "        ConfusionMatrix = np.array([[9, 5], [1, 4]])\n",
    "        FalseAlarms = EvalFalseAlarms(ConfusionMatrix)\n",
    "        self.assertAlmostEqual(FalseAlarms, 0.263, delta = 1e-3)\n",
    "\n",
    "    def test_FalseRejections1(self):\n",
    "        ConfusionMatrix = np.array([[7, 2], [3, 6]])\n",
    "        FalseRejections = EvalFalseRejections(ConfusionMatrix)\n",
    "        self.assertAlmostEqual(FalseRejections, 0.167, delta = 1e-3)\n",
    "\n",
    "    def test_FalseRejections2(self):\n",
    "        ConfusionMatrix = np.array([[9, 5], [1, 4]])\n",
    "        FalseRejections = EvalFalseRejections(ConfusionMatrix)\n",
    "        self.assertAlmostEqual(FalseRejections, 0.053, delta = 1e-3)\n",
    "\n",
    "    def test_F1Score1(self):\n",
    "        ConfusionMatrix = np.array([[7, 2], [3, 6]])\n",
    "        F1Score = EvalF1Score(ConfusionMatrix)\n",
    "        self.assertAlmostEqual(F1Score, 0.724, delta = 1e-3)\n",
    "\n",
    "    def test_F1Score2(self):\n",
    "        ConfusionMatrix = np.array([[9, 5], [1, 4]])\n",
    "        F1Score = EvalF1Score(ConfusionMatrix)\n",
    "        self.assertAlmostEqual(F1Score, 0.696, delta = 1e-3)\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db235211-8607-4951-b253-c51eec0a53e6",
   "metadata": {},
   "source": [
    "## Exam preparation\n",
    "\n",
    "1) You have collected images from three different animals in wildlife: apes (356 images), boars (987 images) and unicorns (1 image taken after a wild student party). What is the accuracy of the simplest possible classificator and what is the output of this simplest possible classificator?\n",
    "\n",
    "2) The following confusion matrix CM is given. Evaluate the accuracy for this confusion matrix and the recall and precision for each class. Which accuracy can be acchieved by the simplest possible classificator? Is this data set balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee896be6-f9a2-4842-b338-a052fd7207a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8 1 2]\n",
      " [0 5 3]\n",
      " [3 2 5]]\n"
     ]
    }
   ],
   "source": [
    "CM = np.array([[8, 1, 2], [0, 5, 3], [3, 2, 5]])\n",
    "print(CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf59490f-ba2e-4598-a59a-33f47a6d995c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
