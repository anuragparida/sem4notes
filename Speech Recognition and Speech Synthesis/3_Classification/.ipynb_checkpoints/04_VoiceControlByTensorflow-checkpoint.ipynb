{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dceb08c-4c62-4bb9-954a-d339fa9b5bff",
   "metadata": {},
   "source": [
    "# Classification for voice control\n",
    "The current implementation of the MOPS uses a combination of Hidden Markov Models (HMM) and Gaussian Mixture Models (GMM) to classify the commands detected by the voice activity detection. In this notebook, Tensorflow should be used instead, in order to get practice in applying Tensorflow to classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcd18fc0-945c-4102-bdc1-65ad9d20cef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installed version of Tensorflow:  2.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "os.chdir('../Python')\n",
    "import TrainingsDataInterface\n",
    "import TrainingsInterface\n",
    "import Train\n",
    "import DatasetAugmentation\n",
    "import Constants\n",
    "\n",
    "TempFolder = \"NeuralNetworks/VoiceControlByTensorflow\"\n",
    "FilenameData = TempFolder + '/Data.npz'\n",
    "try:\n",
    "    os.mkdir(TempFolder)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print('installed version of Tensorflow: ', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2124e492-4251-4dab-84b3-3c92c3b88938",
   "metadata": {},
   "source": [
    "## Evaluate the input data\n",
    "In the following codeblock, the software of the MOPS is used to perform the following three steps:\n",
    "\n",
    "1) Read the audio data: x, Fs, bits = ATrainingsDataInterface.GetWaveOfCommandInstance(CommandIndex, InstanceIndex)\n",
    "\n",
    "2) Optionally apply dataset augmentation: y, Timestretchfactor = ADatasetAugmentation.GenerateSingleDistortion(DistortionIndex)\n",
    "\n",
    "3) Evaluate the MFCC, $\\Delta$MFCC und $\\Delta\\Delta$MFCC: Feature = TrainingsInterface.SamplesToFeature(z, Fs)\n",
    "\n",
    "The evaluated features are stored in three different datasets:\n",
    "\n",
    "1) train_images,\n",
    "\n",
    "2) validation_images and\n",
    "\n",
    "3) test_images.\n",
    "\n",
    "The corresponding indices of the commands is stored in the ground truth\n",
    "\n",
    "1) train_labels,\n",
    "\n",
    "2) validation_labels and\n",
    "\n",
    "3) test_labels.\n",
    "\n",
    "Warning: This Code is slow. It need to be called only if the trainingsdata need to be re-evaluated. Otherwise, everything is loaded in the next code block directly from the hard disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c0aca6-7e0c-4e5c-9161-dd7e5bc5fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 47/47 [01:28<00:00,  1.87s/it]\n"
     ]
    }
   ],
   "source": [
    "AudioDataLengthInMilliseconds = Constants.theConstants.getWordLengthInMilliseconds()\n",
    "NumberOfTestSamples = 20\n",
    "NumberOfValidationSamples = 20\n",
    "\n",
    "ATrainingsDataInterface = TrainingsDataInterface.CTrainingsDataInterface()\n",
    "def GetNumberOfTrainingsData():\n",
    "    res = 0\n",
    "    for CommandIndex in range(ATrainingsDataInterface.GetNumberOfCommands()):\n",
    "        command = ATrainingsDataInterface.GetCommandString(CommandIndex)\n",
    "        if command in Train.VOCABULARY:\n",
    "            NewSamples = ATrainingsDataInterface.GetNumberOfCommandInstances(CommandIndex)\n",
    "            NewSamples -= NumberOfTestSamples\n",
    "            NewSamples -= NumberOfValidationSamples\n",
    "            assert NewSamples > 0, str('not enough training samples for command ' + command)\n",
    "            res += NewSamples\n",
    "    return res\n",
    "\n",
    "def GetAudioWithConstantLength(x, Fs):\n",
    "    LengthInSamples = int(AudioDataLengthInMilliseconds * Fs / 1000)\n",
    "    if x.shape[0] < LengthInSamples:\n",
    "        y = np.concatenate((x, np.zeros((LengthInSamples - x.shape[0]))), axis = 0)\n",
    "    else:\n",
    "        E_cumsum = np.cumsum(x**2)\n",
    "        tmp = E_cumsum[LengthInSamples:]\n",
    "        tmp -= E_cumsum[:tmp.shape[0]]\n",
    "        MaxIndex = np.argmax(tmp)\n",
    "        y = x[MaxIndex:MaxIndex + LengthInSamples]\n",
    "    assert np.abs(y.shape[0] - LengthInSamples) < 1e-1, 'wrong output length'\n",
    "    return y\n",
    "\n",
    "def IsTraining(InstanceIndex):\n",
    "    return not (IsTest(InstanceIndex) or IsValidation(InstanceIndex))\n",
    "\n",
    "def IsValidation(InstanceIndex):\n",
    "    return (not IsTest(InstanceIndex)) and (InstanceIndex < (NumberOfTestSamples + NumberOfValidationSamples))\n",
    "\n",
    "def IsTest(InstanceIndex):\n",
    "    return InstanceIndex < NumberOfTestSamples\n",
    "\n",
    "def EvaluateAllData():\n",
    "    Constants.theConstants.SetUseVAD(False)\n",
    "    train_images = None\n",
    "    train_counter = 0\n",
    "    test_counter = 0\n",
    "    validation_counter = 0\n",
    "    for CommandIndex in tqdm(range(ATrainingsDataInterface.GetNumberOfCommands())):\n",
    "        command = ATrainingsDataInterface.GetCommandString(CommandIndex)\n",
    "        if command in Train.VOCABULARY:\n",
    "            for n in range(len(Train.VOCABULARY)):\n",
    "                if Train.VOCABULARY[n] == command:\n",
    "                    commandlabel = n\n",
    "            for InstanceIndex in range(ATrainingsDataInterface.GetNumberOfCommandInstances(CommandIndex)):\n",
    "                x, Fs, bits = ATrainingsDataInterface.GetWaveOfCommandInstance(CommandIndex, InstanceIndex)\n",
    "                assert np.abs(Constants.theConstants.getSamplingFrequencyMicrofone() - Fs) < 1e-3, 'wrong sampling rate'\n",
    "                ADatasetAugmentation = DatasetAugmentation.CAudioDatasetAugmentation(x, Fs)\n",
    "                NumberOfDistortions = 1#ADatasetAugmentation.GetNumberOfResults()\n",
    "                if IsTraining(InstanceIndex):\n",
    "                    MaxDistortionIndex = NumberOfDistortions\n",
    "                else:\n",
    "                    MaxDistortionIndex = 1\n",
    "                for DistortionIndex in range(MaxDistortionIndex):\n",
    "                    y, Timestretchfactor = ADatasetAugmentation.GenerateSingleDistortion(DistortionIndex)\n",
    "                    z = GetAudioWithConstantLength(y, Fs)\n",
    "                    Feature = TrainingsInterface.SamplesToFeature(z, Fs)\n",
    "                    if train_images is None:\n",
    "                        train_images = np.zeros((GetNumberOfTrainingsData()*NumberOfDistortions, Feature.shape[0], Feature.shape[1]))\n",
    "                        test_images = np.zeros((NumberOfTestSamples*len(Train.VOCABULARY), Feature.shape[0], Feature.shape[1]))\n",
    "                        validation_images = np.zeros((NumberOfValidationSamples*len(Train.VOCABULARY), Feature.shape[0], Feature.shape[1]))\n",
    "                        train_labels = np.zeros((train_images.shape[0]))\n",
    "                        test_labels = np.zeros((test_images.shape[0]))\n",
    "                        validation_labels = np.zeros((validation_images.shape[0]))\n",
    "                    if IsTraining(InstanceIndex):\n",
    "                        train_images[train_counter, :, :] = Feature\n",
    "                        train_labels[train_counter] = commandlabel\n",
    "                        train_counter += 1   \n",
    "                    elif IsTest(InstanceIndex):\n",
    "                        test_images[test_counter, :, :] = Feature\n",
    "                        test_labels[test_counter] = commandlabel\n",
    "                        test_counter += 1   \n",
    "                    else:\n",
    "                        validation_images[validation_counter, :, :] = Feature\n",
    "                        validation_labels[validation_counter] = commandlabel\n",
    "                        validation_counter += 1                         \n",
    "    return train_images, train_labels, test_images, test_labels, validation_images, validation_labels\n",
    "\n",
    "train_images, train_labels, test_images, test_labels, validation_images, validation_labels = EvaluateAllData()\n",
    "np.savez(FilenameData, x0 = train_images, x1 = train_labels, x2 = test_images, x3 = test_labels, x4 = validation_images, x5 = validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dbb5ec-a06d-4b15-acc4-7f59a8a47969",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "In the following code block the pre-evaluated trainings-, validation- and testdata is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44f750b2-3808-407d-8384-88e4a30bbfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainings samples:  1565\n",
      "number of validation samples:  180\n",
      "number of test samples:  180\n"
     ]
    }
   ],
   "source": [
    "def GetAllData():    \n",
    "    try:\n",
    "        data = np.load(FilenameData)\n",
    "        train_images = data['x0']\n",
    "        train_labels = data['x1']\n",
    "        test_images = data['x2']\n",
    "        test_labels = data['x3']\n",
    "        validation_images = data['x4']\n",
    "        validation_labels = data['x5']\n",
    "    except:\n",
    "        train_images, train_labels, test_images, test_labels, validation_images, validation_labels = EvaluateAllData()\n",
    "        np.savez(Filename, x0 = train_images, x1 = train_labels, x2 = test_images, x3 = test_labels, x4 = validation_images, x5 = validation_labels)\n",
    "    return train_images, train_labels, test_images, test_labels, validation_images, validation_labels\n",
    "\n",
    "train_images, train_labels, test_images, test_labels, validation_images, validation_labels = GetAllData()\n",
    "print('number of trainings samples: ', train_images.shape[0])\n",
    "print('number of validation samples: ', validation_images.shape[0])\n",
    "print('number of test samples: ', test_images.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42202a5d-2dac-4fc4-9ee5-7161977f8d7e",
   "metadata": {},
   "source": [
    "## Output and loss\n",
    "In a classification task, you want to have a last layer in your neural network, which outputs values, which can be interpreted as a probability distribution. This is done by the so called softmax layer:\n",
    "\n",
    "a) Evaluate the exponential function.\n",
    "\n",
    "b) Normalize the output values to a sum of $1$.\n",
    "\n",
    "$y_j = \\frac{e^{x_j}}{\\sum_{l=0}^{J-1} e^{x_l}}$\n",
    "\n",
    "The softmax layer is usually combined with the cross entropy as loss function:\n",
    "\n",
    "$L=-\\sum_j o_j\\cdot\\log z_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5153f5f-3f24-4438-8657-c88d110bd1f9",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "In the following block, the model architecture is defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67b360eb-0b4b-4fbc-9211-7ad8cdaa45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(train_images.shape[1], train_images.shape[2])),\n",
    "    tf.keras.layers.Dense(units = 100, activation='LeakyReLU'),\n",
    "    tf.keras.layers.Dense(units = 50, activation='LeakyReLU'),\n",
    "    tf.keras.layers.Dense(len(Train.VOCABULARY), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814aacca-e43d-46bb-9c8a-fa4d86d8667d",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "The training should stop, when the accuracy is no longer increasing. For this, the early stopping callback can be used. The accuracy of the trainingsdata is usually strictly increasing. Therefore, the validation accuracy is a good control measure for early stopping.\n",
    "\n",
    "As an additional callback, the training process is stored in so called checkpoints, which can be reloaded for future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84250bac-7ef4-40c1-8938-1b1b96208077",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = TempFolder + \"/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cbCheckpoints = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "cbEarlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "\n",
    "try:\n",
    "    model.load_weights(checkpoint_path)\n",
    "except:\n",
    "    print('problem loading old weights, starting with scratch new network')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46543c-d7e7-4961-ab4c-f27238bdb82c",
   "metadata": {},
   "source": [
    "## Training\n",
    "The training can be applied to a very large number of epochs, due to the usage of the early stopping callback. After finishing the training, the test accuracy is evaluated:\n",
    "\n",
    "The test data is not seen by the algorithm during the training process. Therefore, the test accuracy is a good measure of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29ebdcb6-ca51-497c-882b-9e91907242bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.5563 - accuracy: 0.8423\n",
      "Epoch 1: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt\n",
      "49/49 [==============================] - 2s 17ms/step - loss: 0.5657 - accuracy: 0.8383 - val_loss: 0.9811 - val_accuracy: 0.6611\n",
      "Epoch 2/1000\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.5690 - accuracy: 0.8374\n",
      "Epoch 2: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.5659 - accuracy: 0.8326 - val_loss: 0.9754 - val_accuracy: 0.6333\n",
      "Epoch 3/1000\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.5067 - accuracy: 0.8551\n",
      "Epoch 3: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.5107 - accuracy: 0.8537 - val_loss: 1.1934 - val_accuracy: 0.5611\n",
      "Epoch 4/1000\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.4952 - accuracy: 0.8465\n",
      "Epoch 4: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.5010 - accuracy: 0.8441 - val_loss: 1.0171 - val_accuracy: 0.6167\n",
      "Epoch 5/1000\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.4850 - accuracy: 0.8597\n",
      "Epoch 5: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt\n",
      "49/49 [==============================] - 1s 10ms/step - loss: 0.4836 - accuracy: 0.8581 - val_loss: 0.9891 - val_accuracy: 0.6500\n",
      "Epoch 6/1000\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.4409 - accuracy: 0.8723\n",
      "Epoch 6: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 0.4437 - accuracy: 0.8722 - val_loss: 0.9319 - val_accuracy: 0.6389\n",
      "training finished after  6  epochs\n",
      "6/6 - 0s - loss: 0.8322 - accuracy: 0.6889 - 47ms/epoch - 8ms/step\n",
      "\n",
      "Test accuracy: 0.6888889074325562\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=1000,\n",
    "                    validation_data=(validation_images, validation_labels),\n",
    "                    callbacks=[cbEarlyStopping, cbCheckpoints], verbose = 1)\n",
    "print('training finished after ', len(history.history['loss']), ' epochs')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e5849b-6348-47a5-9592-e56ae2cc6a74",
   "metadata": {},
   "source": [
    "## Programming exercise\n",
    "\n",
    "The log-mel spectrogram as input feature can be interpreted as an image. Search the web for standard image classification layers and models. Implement model as a sequential set of layers for typical image classification tasks.\n",
    "Compile model and fit it in order to increase the test-accuracy to the maximum possible value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9396a3b-4c6f-4825-b591-4490017c7155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 2.1940 - accuracy: 0.1309\n",
      "Epoch 1: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 3s 50ms/step - loss: 2.1939 - accuracy: 0.1310 - val_loss: 2.1926 - val_accuracy: 0.1111\n",
      "Epoch 2/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 2.1063 - accuracy: 0.1784\n",
      "Epoch 2: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 45ms/step - loss: 2.1014 - accuracy: 0.1789 - val_loss: 1.9572 - val_accuracy: 0.2722\n",
      "Epoch 3/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 1.7705 - accuracy: 0.3789\n",
      "Epoch 3: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 1.7656 - accuracy: 0.3802 - val_loss: 1.8287 - val_accuracy: 0.4056\n",
      "Epoch 4/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 1.5953 - accuracy: 0.4590\n",
      "Epoch 4: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 1.5957 - accuracy: 0.4581 - val_loss: 1.7194 - val_accuracy: 0.3667\n",
      "Epoch 5/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.3875 - accuracy: 0.5284\n",
      "Epoch 5: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 42ms/step - loss: 1.3875 - accuracy: 0.5284 - val_loss: 1.5447 - val_accuracy: 0.4833\n",
      "Epoch 6/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1919 - accuracy: 0.6096\n",
      "Epoch 6: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 1.1919 - accuracy: 0.6096 - val_loss: 1.3641 - val_accuracy: 0.5667\n",
      "Epoch 7/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.9914 - accuracy: 0.6953\n",
      "Epoch 7: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 42ms/step - loss: 0.9897 - accuracy: 0.6958 - val_loss: 1.1978 - val_accuracy: 0.6500\n",
      "Epoch 8/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.7935 - accuracy: 0.7422\n",
      "Epoch 8: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.7929 - accuracy: 0.7425 - val_loss: 1.0754 - val_accuracy: 0.6167\n",
      "Epoch 9/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.7228 - accuracy: 0.7585\n",
      "Epoch 9: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 42ms/step - loss: 0.7228 - accuracy: 0.7585 - val_loss: 0.9570 - val_accuracy: 0.6667\n",
      "Epoch 10/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6233 - accuracy: 0.8064\n",
      "Epoch 10: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 45ms/step - loss: 0.6233 - accuracy: 0.8064 - val_loss: 0.8851 - val_accuracy: 0.7167\n",
      "Epoch 11/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5443 - accuracy: 0.8294\n",
      "Epoch 11: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 44ms/step - loss: 0.5443 - accuracy: 0.8294 - val_loss: 0.8534 - val_accuracy: 0.7389\n",
      "Epoch 12/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4915 - accuracy: 0.8473\n",
      "Epoch 12: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 44ms/step - loss: 0.4915 - accuracy: 0.8473 - val_loss: 0.7606 - val_accuracy: 0.7444\n",
      "Epoch 13/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4629 - accuracy: 0.8550\n",
      "Epoch 13: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 42ms/step - loss: 0.4629 - accuracy: 0.8550 - val_loss: 0.7023 - val_accuracy: 0.7556\n",
      "Epoch 14/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4219 - accuracy: 0.8696\n",
      "Epoch 14: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 44ms/step - loss: 0.4219 - accuracy: 0.8696 - val_loss: 0.6749 - val_accuracy: 0.7889\n",
      "Epoch 15/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3811 - accuracy: 0.8773\n",
      "Epoch 15: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.3811 - accuracy: 0.8773 - val_loss: 0.7352 - val_accuracy: 0.7667\n",
      "Epoch 16/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3520 - accuracy: 0.8946\n",
      "Epoch 16: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 42ms/step - loss: 0.3520 - accuracy: 0.8946 - val_loss: 0.6669 - val_accuracy: 0.7722\n",
      "Epoch 17/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3312 - accuracy: 0.9035\n",
      "Epoch 17: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 42ms/step - loss: 0.3312 - accuracy: 0.9035 - val_loss: 0.6871 - val_accuracy: 0.8000\n",
      "Epoch 18/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3238 - accuracy: 0.8965\n",
      "Epoch 18: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.3238 - accuracy: 0.8965 - val_loss: 0.5798 - val_accuracy: 0.7833\n",
      "Epoch 19/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.2725 - accuracy: 0.9169\n",
      "Epoch 19: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.2725 - accuracy: 0.9169 - val_loss: 0.5855 - val_accuracy: 0.8056\n",
      "Epoch 20/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.2493 - accuracy: 0.9240\n",
      "Epoch 20: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.2493 - accuracy: 0.9240 - val_loss: 0.5456 - val_accuracy: 0.8278\n",
      "Epoch 21/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.2535 - accuracy: 0.9240\n",
      "Epoch 21: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.2535 - accuracy: 0.9240 - val_loss: 0.6301 - val_accuracy: 0.8111\n",
      "Epoch 22/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.2241 - accuracy: 0.9304\n",
      "Epoch 22: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 42ms/step - loss: 0.2241 - accuracy: 0.9304 - val_loss: 0.6694 - val_accuracy: 0.7833\n",
      "Epoch 23/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.2388 - accuracy: 0.9245\n",
      "Epoch 23: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.2362 - accuracy: 0.9252 - val_loss: 0.6842 - val_accuracy: 0.7833\n",
      "Epoch 24/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.2061 - accuracy: 0.9348\n",
      "Epoch 24: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.2061 - accuracy: 0.9348 - val_loss: 0.5313 - val_accuracy: 0.8111\n",
      "Epoch 25/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1722 - accuracy: 0.9476\n",
      "Epoch 25: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 45ms/step - loss: 0.1722 - accuracy: 0.9476 - val_loss: 0.6280 - val_accuracy: 0.8111\n",
      "Epoch 26/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1682 - accuracy: 0.9527\n",
      "Epoch 26: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.1682 - accuracy: 0.9527 - val_loss: 0.5532 - val_accuracy: 0.8333\n",
      "Epoch 27/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1620 - accuracy: 0.9518\n",
      "Epoch 27: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.1606 - accuracy: 0.9527 - val_loss: 0.4470 - val_accuracy: 0.8333\n",
      "Epoch 28/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1467 - accuracy: 0.9610\n",
      "Epoch 28: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 44ms/step - loss: 0.1467 - accuracy: 0.9610 - val_loss: 0.6233 - val_accuracy: 0.8000\n",
      "Epoch 29/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1430 - accuracy: 0.9570\n",
      "Epoch 29: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.1433 - accuracy: 0.9572 - val_loss: 0.4993 - val_accuracy: 0.8667\n",
      "Epoch 30/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1184 - accuracy: 0.9655\n",
      "Epoch 30: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.1184 - accuracy: 0.9655 - val_loss: 0.5460 - val_accuracy: 0.8333\n",
      "Epoch 31/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9655\n",
      "Epoch 31: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 42ms/step - loss: 0.1156 - accuracy: 0.9655 - val_loss: 0.4917 - val_accuracy: 0.8556\n",
      "Epoch 32/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9719\n",
      "Epoch 32: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.1001 - accuracy: 0.9719 - val_loss: 0.5434 - val_accuracy: 0.8389\n",
      "Epoch 33/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.0924 - accuracy: 0.9753\n",
      "Epoch 33: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.0925 - accuracy: 0.9751 - val_loss: 0.4949 - val_accuracy: 0.8611\n",
      "Epoch 34/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0906 - accuracy: 0.9744\n",
      "Epoch 34: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.0906 - accuracy: 0.9744 - val_loss: 0.5106 - val_accuracy: 0.8722\n",
      "Epoch 35/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1034 - accuracy: 0.9720\n",
      "Epoch 35: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 44ms/step - loss: 0.1028 - accuracy: 0.9719 - val_loss: 0.5644 - val_accuracy: 0.8556\n",
      "Epoch 36/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0885 - accuracy: 0.9738\n",
      "Epoch 36: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 42ms/step - loss: 0.0885 - accuracy: 0.9738 - val_loss: 0.5468 - val_accuracy: 0.8444\n",
      "Epoch 37/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0948 - accuracy: 0.9712\n",
      "Epoch 37: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.0948 - accuracy: 0.9712 - val_loss: 0.6476 - val_accuracy: 0.8389\n",
      "Epoch 38/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.9766\n",
      "Epoch 38: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.0843 - accuracy: 0.9757 - val_loss: 0.5824 - val_accuracy: 0.8556\n",
      "Epoch 39/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0843 - accuracy: 0.9693\n",
      "Epoch 39: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 42ms/step - loss: 0.0843 - accuracy: 0.9693 - val_loss: 0.6068 - val_accuracy: 0.8556\n",
      "Epoch 40/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9764\n",
      "Epoch 40: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 43ms/step - loss: 0.0783 - accuracy: 0.9764 - val_loss: 0.5387 - val_accuracy: 0.8500\n",
      "Epoch 41/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.0642 - accuracy: 0.9824\n",
      "Epoch 41: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 44ms/step - loss: 0.0643 - accuracy: 0.9827 - val_loss: 0.5053 - val_accuracy: 0.8722\n",
      "Epoch 42/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0602 - accuracy: 0.9840\n",
      "Epoch 42: saving model to NeuralNetworks/VoiceControlByTensorflow\\cp.ckpt.weights.h5\n",
      "49/49 [==============================] - 2s 44ms/step - loss: 0.0602 - accuracy: 0.9840 - val_loss: 0.5170 - val_accuracy: 0.8556\n",
      "training finished after  42  epochs\n",
      "6/6 - 0s - loss: 0.3374 - accuracy: 0.9000 - 125ms/epoch - 21ms/step\n",
      "\n",
      "Test accuracy: 0.8999999761581421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_1 (__main__.TestProgrammingExercise.test_1) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 - 0s - loss: 0.3374 - accuracy: 0.9000 - 112ms/epoch - 19ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.175s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.8999999761581421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x27073553050>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = None\n",
    "### solution begins\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "tf.keras.layers.Input(shape=(train_images.shape[1], train_images.shape[2], 1)), # assuming grayscale images\n",
    "tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "tf.keras.layers.Flatten(),\n",
    "tf.keras.layers.Dense(128, activation='relu'),\n",
    "tf.keras.layers.Dense(len(Train.VOCABULARY), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "checkpoint_path = TempFolder + \"/cp.ckpt.weights.h5\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cbCheckpoints = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "save_weights_only=True,\n",
    "verbose=1)\n",
    "cbEarlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=8)\n",
    "\n",
    "try:\n",
    " model.load_weights(checkpoint_path)\n",
    "except:\n",
    " print('problem loading old weights, starting with scratch new network')\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=1000,\n",
    "validation_data=(validation_images, validation_labels),\n",
    "callbacks=[cbEarlyStopping, cbCheckpoints], verbose = 1)\n",
    "print('training finished after ', len(history.history['loss']), ' epochs')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "import unittest\n",
    "\n",
    "class TestProgrammingExercise(unittest.TestCase):\n",
    "\n",
    "    def test_1(self):\n",
    "        test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "        print('\\nTest accuracy:', test_acc)        \n",
    "        self.assertGreater(test_acc, 0.8)\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c2181e-1255-4d13-bde4-0f8bd242d72c",
   "metadata": {},
   "source": [
    "## Exam preparation\n",
    "\n",
    "1) Assuming a dataset with nine different classes and the corresponding counts: label0: 136 times, label1: 180 times, label2: 180 times, label3: 180 times, label4: 180 times, label5: 213 times, label6: 180 times, label7: 136 times and label 8: 180 times. Evaluate the accuracy of the simplest possible classificator. Is the dataset balanced?\n",
    "\n",
    "2) Evaluate the derivative of the softmax layer: $\\frac{dy_j}{dx_i}$.\n",
    "\n",
    "3) Evaluate the derivative of the loss given by the cross entropy to the input of the softmax layer: $\\frac{dL}{dx_i}=\\frac{dL}{dy_j}\\cdot\\frac{dy_j}{dx_i}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da51364-e9f3-4fd7-8030-eb1842d0aa33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
