{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb82dc9-3f98-4863-a789-dab91ef77c43",
   "metadata": {},
   "source": [
    "# Classification for voice control\n",
    "The current implementation of the MOPS uses a combination of Hidden Markov Models (HMM) and Gaussian Mixture Models (GMM) to classify the commands detected by the voice activity detection. In this notebook, Keras should be used instead, in order to get practice in applying Tensorflow to classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcd18fc0-945c-4102-bdc1-65ad9d20cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "os.chdir('../Python')\n",
    "import TrainingsDataInterface\n",
    "import TrainingsInterface\n",
    "import Train\n",
    "import DatasetAugmentation\n",
    "import Constants\n",
    "\n",
    "TempFolder = \"NeuralNetworks/VoiceControlByKeras\"\n",
    "FilenameData = TempFolder + '/Data.npz'\n",
    "try:\n",
    "    os.mkdir(TempFolder)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2124e492-4251-4dab-84b3-3c92c3b88938",
   "metadata": {},
   "source": [
    "## Evaluate the input data\n",
    "In the following codeblock, the software of the MOPS is used to perform the following three steps:\n",
    "\n",
    "1) Read the audio data: x, Fs, bits = ATrainingsDataInterface.GetWaveOfCommandInstance(CommandIndex, InstanceIndex)\n",
    "\n",
    "2) Optionally apply dataset augmentation: y, Timestretchfactor = ADatasetAugmentation.GenerateSingleDistortion(DistortionIndex)\n",
    "\n",
    "3) Evaluate the MFCC, $\\Delta$MFCC und $\\Delta\\Delta$MFCC: Feature = TrainingsInterface.SamplesToFeature(z, Fs)\n",
    "\n",
    "The evaluated features are stored in three different datasets:\n",
    "\n",
    "1) train_images,\n",
    "\n",
    "2) validation_images and\n",
    "\n",
    "3) test_images.\n",
    "\n",
    "The corresponding indices of the commands is stored in the ground truth\n",
    "\n",
    "1) train_labels,\n",
    "\n",
    "2) validation_labels and\n",
    "\n",
    "3) test_labels.\n",
    "\n",
    "Warning: This Code is slow. It need to be called only if the trainingsdata need to be re-evaluated. Otherwise, everything is loaded in the next code block directly from the hard disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25c0aca6-7e0c-4e5c-9161-dd7e5bc5fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 47/47 [01:02<00:00,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "AudioDataLengthInMilliseconds = Constants.theConstants.getWordLengthInMilliseconds()\n",
    "NumberOfTestSamples = 20\n",
    "NumberOfValidationSamples = 20\n",
    "\n",
    "ATrainingsDataInterface = TrainingsDataInterface.CTrainingsDataInterface()\n",
    "def GetNumberOfTrainingsData():\n",
    "    res = 0\n",
    "    for CommandIndex in range(ATrainingsDataInterface.GetNumberOfCommands()):\n",
    "        command = ATrainingsDataInterface.GetCommandString(CommandIndex)\n",
    "        if command in Train.VOCABULARY:\n",
    "            NewSamples = ATrainingsDataInterface.GetNumberOfCommandInstances(CommandIndex)\n",
    "            NewSamples -= NumberOfTestSamples\n",
    "            NewSamples -= NumberOfValidationSamples\n",
    "            assert NewSamples > 0, str('not enough training samples for command ' + command)\n",
    "            res += NewSamples\n",
    "    return res\n",
    "\n",
    "def GetAudioWithConstantLength(x, Fs):\n",
    "    LengthInSamples = int(AudioDataLengthInMilliseconds * Fs / 1000)\n",
    "    if x.shape[0] < LengthInSamples:\n",
    "        y = np.concatenate((x, np.zeros((LengthInSamples - x.shape[0]))), axis = 0)\n",
    "    else:\n",
    "        E_cumsum = np.cumsum(x**2)\n",
    "        tmp = E_cumsum[LengthInSamples:]\n",
    "        tmp -= E_cumsum[:tmp.shape[0]]\n",
    "        MaxIndex = np.argmax(tmp)\n",
    "        y = x[MaxIndex:MaxIndex + LengthInSamples]\n",
    "    assert np.abs(y.shape[0] - LengthInSamples) < 1e-1, 'wrong output length'\n",
    "    return y\n",
    "\n",
    "def IsTraining(InstanceIndex):\n",
    "    return not (IsTest(InstanceIndex) or IsValidation(InstanceIndex))\n",
    "\n",
    "def IsValidation(InstanceIndex):\n",
    "    return (not IsTest(InstanceIndex)) and (InstanceIndex < (NumberOfTestSamples + NumberOfValidationSamples))\n",
    "\n",
    "def IsTest(InstanceIndex):\n",
    "    return InstanceIndex < NumberOfTestSamples\n",
    "\n",
    "def EvaluateAllData():\n",
    "    Constants.theConstants.SetUseVAD(False)\n",
    "    train_images = None\n",
    "    train_counter = 0\n",
    "    test_counter = 0\n",
    "    validation_counter = 0\n",
    "    for CommandIndex in tqdm(range(ATrainingsDataInterface.GetNumberOfCommands())):\n",
    "        command = ATrainingsDataInterface.GetCommandString(CommandIndex)\n",
    "        if command in Train.VOCABULARY:\n",
    "            for n in range(len(Train.VOCABULARY)):\n",
    "                if Train.VOCABULARY[n] == command:\n",
    "                    commandlabel = n\n",
    "            for InstanceIndex in range(ATrainingsDataInterface.GetNumberOfCommandInstances(CommandIndex)):\n",
    "                x, Fs, bits = ATrainingsDataInterface.GetWaveOfCommandInstance(CommandIndex, InstanceIndex)\n",
    "                assert np.abs(Constants.theConstants.getSamplingFrequencyMicrofone() - Fs) < 1e-3, 'wrong sampling rate'\n",
    "                ADatasetAugmentation = DatasetAugmentation.CAudioDatasetAugmentation(x, Fs)\n",
    "                NumberOfDistortions = 1#ADatasetAugmentation.GetNumberOfResults()\n",
    "                if IsTraining(InstanceIndex):\n",
    "                    MaxDistortionIndex = NumberOfDistortions\n",
    "                else:\n",
    "                    MaxDistortionIndex = 1\n",
    "                for DistortionIndex in range(MaxDistortionIndex):\n",
    "                    y, Timestretchfactor = ADatasetAugmentation.GenerateSingleDistortion(DistortionIndex)\n",
    "                    z = GetAudioWithConstantLength(y, Fs)\n",
    "                    Feature = TrainingsInterface.SamplesToFeature(z, Fs)\n",
    "                    if train_images is None:\n",
    "                        train_images = np.zeros((GetNumberOfTrainingsData()*NumberOfDistortions, Feature.shape[0], Feature.shape[1]))\n",
    "                        test_images = np.zeros((NumberOfTestSamples*len(Train.VOCABULARY), Feature.shape[0], Feature.shape[1]))\n",
    "                        validation_images = np.zeros((NumberOfValidationSamples*len(Train.VOCABULARY), Feature.shape[0], Feature.shape[1]))\n",
    "                        train_labels = np.zeros((train_images.shape[0]))\n",
    "                        test_labels = np.zeros((test_images.shape[0]))\n",
    "                        validation_labels = np.zeros((validation_images.shape[0]))\n",
    "                    if IsTraining(InstanceIndex):\n",
    "                        train_images[train_counter, :, :] = Feature\n",
    "                        train_labels[train_counter] = commandlabel\n",
    "                        train_counter += 1   \n",
    "                    elif IsTest(InstanceIndex):\n",
    "                        test_images[test_counter, :, :] = Feature\n",
    "                        test_labels[test_counter] = commandlabel\n",
    "                        test_counter += 1   \n",
    "                    else:\n",
    "                        validation_images[validation_counter, :, :] = Feature\n",
    "                        validation_labels[validation_counter] = commandlabel\n",
    "                        validation_counter += 1                         \n",
    "    return train_images, train_labels, test_images, test_labels, validation_images, validation_labels\n",
    "\n",
    "train_images, train_labels, test_images, test_labels, validation_images, validation_labels = EvaluateAllData()\n",
    "np.savez(FilenameData, x0 = train_images, x1 = train_labels, x2 = test_images, x3 = test_labels, x4 = validation_images, x5 = validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dbb5ec-a06d-4b15-acc4-7f59a8a47969",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "In the following code block the pre-evaluated trainings-, validation- and testdata is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44f750b2-3808-407d-8384-88e4a30bbfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainings samples:  1565\n",
      "number of validation samples:  180\n",
      "number of test samples:  180\n"
     ]
    }
   ],
   "source": [
    "def GetAllData():    \n",
    "    try:\n",
    "        data = np.load(FilenameData)\n",
    "        train_images = data['x0']\n",
    "        train_labels = data['x1']\n",
    "        test_images = data['x2']\n",
    "        test_labels = data['x3']\n",
    "        validation_images = data['x4']\n",
    "        validation_labels = data['x5']\n",
    "    except:\n",
    "        train_images, train_labels, test_images, test_labels, validation_images, validation_labels = EvaluateAllData()\n",
    "        np.savez(Filename, x0 = train_images, x1 = train_labels, x2 = test_images, x3 = test_labels, x4 = validation_images, x5 = validation_labels)\n",
    "    return train_images, train_labels, test_images, test_labels, validation_images, validation_labels\n",
    "\n",
    "train_images, train_labels, test_images, test_labels, validation_images, validation_labels = GetAllData()\n",
    "print('number of trainings samples: ', train_images.shape[0])\n",
    "print('number of validation samples: ', validation_images.shape[0])\n",
    "print('number of test samples: ', test_images.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42202a5d-2dac-4fc4-9ee5-7161977f8d7e",
   "metadata": {},
   "source": [
    "## Output and loss\n",
    "In a classification task, you want to have a last layer in your neural network, which outputs values, which can be interpreted as a probability distribution. This is done by the so called softmax layer:\n",
    "\n",
    "a) Evaluate the exponential function.\n",
    "\n",
    "b) Normalize the output values to a sum of $1$.\n",
    "\n",
    "$y_j = \\frac{e^{x_j}}{\\sum_{l=0}^{J-1} e^{x_l}}$\n",
    "\n",
    "The softmax layer is usually combined with the cross entropy as loss function:\n",
    "\n",
    "$L=-\\sum_j o_j\\cdot\\log z_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5153f5f-3f24-4438-8657-c88d110bd1f9",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "In the following block, the model architecture is defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67b360eb-0b4b-4fbc-9211-7ad8cdaa45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(train_images.shape[1], train_images.shape[2])),\n",
    "    keras.layers.Dense(units = 100, activation='LeakyReLU'),\n",
    "    keras.layers.Dense(units = 50, activation='LeakyReLU'),\n",
    "    keras.layers.Dense(len(Train.VOCABULARY), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814aacca-e43d-46bb-9c8a-fa4d86d8667d",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "The training should stop, when the accuracy is no longer increasing. For this, the early stopping callback can be used. The accuracy of the trainingsdata is usually strictly increasing. Therefore, the validation accuracy is a good control measure for early stopping.\n",
    "\n",
    "As an additional callback, the training process is stored in so called checkpoints, which can be reloaded for future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84250bac-7ef4-40c1-8938-1b1b96208077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem loading old weights, starting with scratch new network\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = TempFolder + \"/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cbCheckpoints = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "cbEarlyStopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "\n",
    "try:\n",
    "    model.load_weights(checkpoint_path)\n",
    "except:\n",
    "    print('problem loading old weights, starting with scratch new network')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46543c-d7e7-4961-ab4c-f27238bdb82c",
   "metadata": {},
   "source": [
    "## Training\n",
    "The training can be applied to a very large number of epochs, due to the usage of the early stopping callback. After finishing the training, the test accuracy is evaluated:\n",
    "\n",
    "The test data is not seen by the algorithm during the training process. Therefore, the test accuracy is a good measure of fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29ebdcb6-ca51-497c-882b-9e91907242bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.1740 - accuracy: 0.1610\n",
      "Epoch 1: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 2s 19ms/step - loss: 2.1740 - accuracy: 0.1610 - val_loss: 2.1641 - val_accuracy: 0.2556\n",
      "Epoch 2/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 2.0506 - accuracy: 0.3275\n",
      "Epoch 2: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 2.0482 - accuracy: 0.3297 - val_loss: 2.0373 - val_accuracy: 0.2667\n",
      "Epoch 3/1000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.8352 - accuracy: 0.3963\n",
      "Epoch 3: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 1.8181 - accuracy: 0.4026 - val_loss: 1.9126 - val_accuracy: 0.3611\n",
      "Epoch 4/1000\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 1.6274 - accuracy: 0.4578\n",
      "Epoch 4: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 1.6116 - accuracy: 0.4633 - val_loss: 1.7353 - val_accuracy: 0.4611\n",
      "Epoch 5/1000\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 1.4108 - accuracy: 0.5874\n",
      "Epoch 5: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 1.4036 - accuracy: 0.5879 - val_loss: 1.6966 - val_accuracy: 0.3500\n",
      "Epoch 6/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.2559 - accuracy: 0.6160\n",
      "Epoch 6: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 1.2559 - accuracy: 0.6160 - val_loss: 1.5376 - val_accuracy: 0.4722\n",
      "Epoch 7/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 1.1273 - accuracy: 0.6588\n",
      "Epoch 7: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 1.1273 - accuracy: 0.6588 - val_loss: 1.3830 - val_accuracy: 0.5722\n",
      "Epoch 8/1000\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 1.0156 - accuracy: 0.7000\n",
      "Epoch 8: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 1.0142 - accuracy: 0.7016 - val_loss: 1.3552 - val_accuracy: 0.5667\n",
      "Epoch 9/1000\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.9420 - accuracy: 0.7102\n",
      "Epoch 9: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.9336 - accuracy: 0.7150 - val_loss: 1.2894 - val_accuracy: 0.5611\n",
      "Epoch 10/1000\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.8632 - accuracy: 0.7452\n",
      "Epoch 10: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.8567 - accuracy: 0.7463 - val_loss: 1.2674 - val_accuracy: 0.5444\n",
      "Epoch 11/1000\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.8000 - accuracy: 0.7664\n",
      "Epoch 11: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.8021 - accuracy: 0.7610 - val_loss: 1.3507 - val_accuracy: 0.5167\n",
      "Epoch 12/1000\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.7369 - accuracy: 0.7784\n",
      "Epoch 12: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.7382 - accuracy: 0.7783 - val_loss: 1.1316 - val_accuracy: 0.6333\n",
      "Epoch 13/1000\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.7043 - accuracy: 0.7857\n",
      "Epoch 13: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.7240 - accuracy: 0.7821 - val_loss: 1.2184 - val_accuracy: 0.6000\n",
      "Epoch 14/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.7006 - accuracy: 0.7741\n",
      "Epoch 14: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.7003 - accuracy: 0.7744 - val_loss: 1.1078 - val_accuracy: 0.6333\n",
      "Epoch 15/1000\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.6379 - accuracy: 0.8049\n",
      "Epoch 15: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.6276 - accuracy: 0.8096 - val_loss: 1.0111 - val_accuracy: 0.6833\n",
      "Epoch 16/1000\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.5902 - accuracy: 0.8333\n",
      "Epoch 16: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.5958 - accuracy: 0.8294 - val_loss: 1.0315 - val_accuracy: 0.6278\n",
      "Epoch 17/1000\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.5408 - accuracy: 0.8438\n",
      "Epoch 17: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.5514 - accuracy: 0.8377 - val_loss: 1.0403 - val_accuracy: 0.6444\n",
      "Epoch 18/1000\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.5130 - accuracy: 0.8414\n",
      "Epoch 18: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.5285 - accuracy: 0.8358 - val_loss: 0.9951 - val_accuracy: 0.6667\n",
      "Epoch 19/1000\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.5266 - accuracy: 0.8376\n",
      "Epoch 19: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.5228 - accuracy: 0.8390 - val_loss: 1.0592 - val_accuracy: 0.5944\n",
      "Epoch 20/1000\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.4804 - accuracy: 0.8570\n",
      "Epoch 20: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.4933 - accuracy: 0.8562 - val_loss: 1.1105 - val_accuracy: 0.6056\n",
      "training finished after  20  epochs\n",
      "6/6 - 0s - loss: 0.9525 - accuracy: 0.6333 - 31ms/epoch - 5ms/step\n",
      "\n",
      "Test accuracy: 0.6333333253860474\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=1000,\n",
    "                    validation_data=(validation_images, validation_labels),\n",
    "                    callbacks=[cbEarlyStopping, cbCheckpoints], verbose = 1)\n",
    "print('training finished after ', len(history.history['loss']), ' epochs')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e5849b-6348-47a5-9592-e56ae2cc6a74",
   "metadata": {},
   "source": [
    "## Programming exercise\n",
    "\n",
    "The log-mel spectrogram as input feature can be interpreted as an image. Search the web for standard image classification layers and models. Implement model as a sequential set of layers for typical image classification tasks.\n",
    "Compile model and fit it in order to increase the test-accuracy to the maximum possible value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9396a3b-4c6f-4825-b591-4490017c7155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 2.1863 - accuracy: 0.1323\n",
      "Epoch 1: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 3s 47ms/step - loss: 2.1842 - accuracy: 0.1348 - val_loss: 2.2030 - val_accuracy: 0.1111\n",
      "Epoch 2/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 2.1039 - accuracy: 0.2858\n",
      "Epoch 2: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 2.1009 - accuracy: 0.2882 - val_loss: 2.0338 - val_accuracy: 0.2944\n",
      "Epoch 3/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 1.7472 - accuracy: 0.4303\n",
      "Epoch 3: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 2s 36ms/step - loss: 1.7388 - accuracy: 0.4332 - val_loss: 1.6746 - val_accuracy: 0.3722\n",
      "Epoch 4/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 1.2077 - accuracy: 0.6478\n",
      "Epoch 4: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 2s 33ms/step - loss: 1.2001 - accuracy: 0.6505 - val_loss: 1.3200 - val_accuracy: 0.5556\n",
      "Epoch 5/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.9211 - accuracy: 0.7207\n",
      "Epoch 5: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 0.9223 - accuracy: 0.7208 - val_loss: 1.0658 - val_accuracy: 0.6444\n",
      "Epoch 6/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.7954 - accuracy: 0.7585\n",
      "Epoch 6: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 0.7944 - accuracy: 0.7578 - val_loss: 1.0040 - val_accuracy: 0.6611\n",
      "Epoch 7/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.7090 - accuracy: 0.7865\n",
      "Epoch 7: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 0.7052 - accuracy: 0.7866 - val_loss: 0.9302 - val_accuracy: 0.7278\n",
      "Epoch 8/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.6419 - accuracy: 0.8086\n",
      "Epoch 8: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 0.6388 - accuracy: 0.8089 - val_loss: 0.9723 - val_accuracy: 0.6667\n",
      "Epoch 9/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.6049 - accuracy: 0.8096\n",
      "Epoch 9: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 2s 32ms/step - loss: 0.6049 - accuracy: 0.8096 - val_loss: 0.9814 - val_accuracy: 0.6889\n",
      "Epoch 10/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5441 - accuracy: 0.8441\n",
      "Epoch 10: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 0.5441 - accuracy: 0.8441 - val_loss: 0.8479 - val_accuracy: 0.7056\n",
      "Epoch 11/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.5126 - accuracy: 0.8530\n",
      "Epoch 11: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 0.5126 - accuracy: 0.8530 - val_loss: 0.6740 - val_accuracy: 0.7667\n",
      "Epoch 12/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.4661 - accuracy: 0.8678\n",
      "Epoch 12: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 0.4681 - accuracy: 0.8671 - val_loss: 0.8669 - val_accuracy: 0.7167\n",
      "Epoch 13/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.4555 - accuracy: 0.8665\n",
      "Epoch 13: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 2s 30ms/step - loss: 0.4644 - accuracy: 0.8633 - val_loss: 0.6734 - val_accuracy: 0.7778\n",
      "Epoch 14/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.4229 - accuracy: 0.8748\n",
      "Epoch 14: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 1s 31ms/step - loss: 0.4229 - accuracy: 0.8748 - val_loss: 0.6369 - val_accuracy: 0.8111\n",
      "Epoch 15/1000\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.3851 - accuracy: 0.8932\n",
      "Epoch 15: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 0.3838 - accuracy: 0.8933 - val_loss: 0.5837 - val_accuracy: 0.8056\n",
      "Epoch 16/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3798 - accuracy: 0.8958\n",
      "Epoch 16: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 1s 30ms/step - loss: 0.3798 - accuracy: 0.8958 - val_loss: 0.6255 - val_accuracy: 0.7833\n",
      "Epoch 17/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3648 - accuracy: 0.8939\n",
      "Epoch 17: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 0.3648 - accuracy: 0.8939 - val_loss: 0.6294 - val_accuracy: 0.8000\n",
      "Epoch 18/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3409 - accuracy: 0.9003\n",
      "Epoch 18: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 2s 31ms/step - loss: 0.3409 - accuracy: 0.9003 - val_loss: 0.6662 - val_accuracy: 0.7667\n",
      "Epoch 19/1000\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.3413 - accuracy: 0.9029\n",
      "Epoch 19: saving model to NeuralNetworks/VoiceControlByKeras\\cp.ckpt\n",
      "49/49 [==============================] - 2s 30ms/step - loss: 0.3413 - accuracy: 0.9029 - val_loss: 0.6953 - val_accuracy: 0.7667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_1 (__main__.TestProgrammingExercise.test_1) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished after  19  epochs\n",
      "6/6 - 0s - loss: 0.5447 - accuracy: 0.8333 - 92ms/epoch - 15ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.142s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.8333333134651184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x208b80dd750>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = None\n",
    "### solution begins\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Rescaling(1.0, input_shape=(train_images.shape[1], train_images.shape[2], 1)),\n",
    "  keras.layers.Conv2D(20, 3, padding='same', activation='LeakyReLU'),\n",
    "  keras.layers.MaxPooling2D(),\n",
    "  keras.layers.Conv2D(40, 3, padding='same', activation='LeakyReLU'),\n",
    "  keras.layers.MaxPooling2D(),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dropout(0.3),\n",
    "  keras.layers.Dense(40, activation='LeakyReLU'),\n",
    "  keras.layers.Dense(len(Train.VOCABULARY), activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels, epochs=1000,\n",
    "                    validation_data=(validation_images, validation_labels),\n",
    "                    callbacks=[cbEarlyStopping, cbCheckpoints], verbose = 1)\n",
    "print('training finished after ', len(history.history['loss']), ' epochs')\n",
    "### solution ends\n",
    "\n",
    "import unittest\n",
    "\n",
    "class TestProgrammingExercise(unittest.TestCase):\n",
    "\n",
    "    def test_1(self):\n",
    "        test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "        print('\\nTest accuracy:', test_acc)        \n",
    "        self.assertGreater(test_acc, 0.8)\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c2181e-1255-4d13-bde4-0f8bd242d72c",
   "metadata": {},
   "source": [
    "## Exam preparation\n",
    "\n",
    "1) Assuming a dataset with nine different classes and the corresponding counts: label0: 136 times, label1: 180 times, label2: 180 times, label3: 180 times, label4: 180 times, label5: 213 times, label6: 180 times, label7: 136 times and label 8: 180 times. Evaluate the accuracy of the simplest possible classificator. Is the dataset balanced?\n",
    "\n",
    "2) Evaluate the derivative of the softmax layer: $\\frac{dy_j}{dx_i}$.\n",
    "\n",
    "3) Evaluate the derivative of the loss given by the cross entropy to the input of the softmax layer: $\\frac{dL}{dx_i}=\\frac{dL}{dy_j}\\cdot\\frac{dy_j}{dx_i}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da51364-e9f3-4fd7-8030-eb1842d0aa33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
